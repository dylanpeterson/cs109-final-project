{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNP ANALYSIS OF 3000 RICE GENOMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f8/Red_Rice_Paddy_field_in_Japan_001.jpg\" width=750 height=500/>\n",
    "\n",
    "\n",
    "Image from: https://upload.wikimedia.org/wikipedia/commons/f/f8/Red_Rice_Paddy_field_in_Japan_001.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [SNP ANALYSIS OF 3000 RICE GENOMES](#SNP-ANALYSIS-OF-3000-RICE-GENOMES)\n",
    "\t* [Overview](#Overview.)\n",
    "        * [Motivation](#Motivation)\n",
    "        * [Related Work](#Related-Work)\n",
    "        * [Initial Questions](#Initial-Questions)\n",
    "    * [Loading Additional Packages](#Loading-Additional-Packages)\n",
    "    * [Scraping From IRGCIS Database](#Scraping-From-IRGCIS-Database)\n",
    "\t\t* [Database Layout](#Database-Layout)\n",
    "\t\t* [Scraping the Data](#Scraping-the-Data)\n",
    "\t\t* [Structuring the Queries](#Structuring-the-Queries)\n",
    "\t\t* [Database Reconstruction](#Database-Reconstruction)\n",
    "        * [Subselecting for AWS Strains](#Subselecting-for-AWS-Strains)\n",
    "            * [EDA of Phenotypic Variables](#EDA-of-Phenotypic-Variables)\n",
    "    * [Feature Based Prediction of Days to Maturity](#Feature-Based-Prediction-of-Days-to-Maturity)\n",
    "        * [Absolute Baseline](#Absolute-Baseline)\n",
    "        * [Strain Baseline](#Strain-Baseline)\n",
    "        * [Strain-Country Baseline Regression](#Strain-Country-Baseline-Regression)\n",
    "        * [Normalized Strain-Country Baseline Regression](#Normalized-Strain-Country-Baseline-Regression)\n",
    "    * [Accessing SNP Data](#Accessing-SNP-Data)\n",
    "        * [Accessing Process](#Accessing-Process)\n",
    "        * [Database Construction](#Database-Construction)\n",
    "        * [EDA of SNP Data](#EDA-of-SNP-Data)\n",
    "    * [Predicting Days to Maturity with SNP Data](#Predicting-Days-to-Maturity-with-SNP-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that, let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normal Imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "# Additional Imports Needed\n",
    "import requests\n",
    "from pyquery import PyQuery as pq\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Additional Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyVCF was downloaded from https://pypi.python.org/pypi/PyVCF/0.6.0 and added to Python libraries by running the setup.py file. The library can be imported by `import vcf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping From IRGCIS Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/IRGCIS_Queries.png\" width=500 height=300/>\n",
    "<img src=\"images/IRGCIS_Sample_Query.png\" width=1000 height=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT ABOUT WHAT THIS IS AND HOW LONG IT TAKES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Query the online database for the requisite data\n",
    "basicinfo=requests.get(\"http://www.irgcis.irri.org:81/grc/TK.exe$Query?DataSource=IRG&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3E%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3C%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID=&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME-OP=ctn&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME=&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY=&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY-OP=%3D&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY=&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE=&GBUSER_TK_MORPH1_2.MAT-OP=%3E%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.MAT-OP=%3C%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRWD-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.GRWD-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.VG-OP=%3D&GBUSER_TK_MORPH1_2.VG=&GBUSER_TK_MORPH1_2.ENDO-OP=%3D&GBUSER_TK_MORPH1_2.ENDO=&GBUSER_TK_MORPH1_2.SCCO_REV-OP=%3D&GBUSER_TK_MORPH1_2.SCCO_REV=&GBUSER_TK_EVAL.BL_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BL_DESCRIPTION=&GBUSER_TK_EVAL.BB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BB_DESCRIPTION=&GBUSER_TK_EVAL.SHB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.SHB_DESCRIPTION=&Output=%2FGRC%2FAccessionID.htm&Limit=-1\")\n",
    "allinfo=requests.get(\"http://www.irgcis.irri.org:81/grc/TK.exe$Query?DataSource=IRG&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3E%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3C%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID=&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME-OP=ctn&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME=&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY=&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY-OP=%3D&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY=&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE=&GBUSER_TK_MORPH1_2.MAT-OP=%3E%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.MAT-OP=%3C%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRWD-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.GRWD-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.VG-OP=%3D&GBUSER_TK_MORPH1_2.VG=&GBUSER_TK_MORPH1_2.ENDO-OP=%3D&GBUSER_TK_MORPH1_2.ENDO=&GBUSER_TK_MORPH1_2.SCCO_REV-OP=%3D&GBUSER_TK_MORPH1_2.SCCO_REV=&GBUSER_TK_EVAL.BL_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BL_DESCRIPTION=&GBUSER_TK_EVAL.BB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BB_DESCRIPTION=&GBUSER_TK_EVAL.SHB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.SHB_DESCRIPTION=&Output=%2FGRC%2FDataOnSearchForm.htm&Limit=-1\")\n",
    "locationinfo=requests.get(\"http://www.irgcis.irri.org:81/grc/TK.exe$Query?DataSource=IRG&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3E%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3C%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID=&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME-OP=ctn&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME=&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY=&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY-OP=%3D&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY=&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE=&GBUSER_TK_MORPH1_2.MAT-OP=%3E%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.MAT-OP=%3C%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRWD-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.GRWD-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.VG-OP=%3D&GBUSER_TK_MORPH1_2.VG=&GBUSER_TK_MORPH1_2.ENDO-OP=%3D&GBUSER_TK_MORPH1_2.ENDO=&GBUSER_TK_MORPH1_2.SCCO_REV-OP=%3D&GBUSER_TK_MORPH1_2.SCCO_REV=&GBUSER_TK_EVAL.BL_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BL_DESCRIPTION=&GBUSER_TK_EVAL.BB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BB_DESCRIPTION=&GBUSER_TK_EVAL.SHB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.SHB_DESCRIPTION=&Output=%2FGRC%2FPassCollectLoc.htm&Limit=-1\")\n",
    "morphoveginfo=requests.get(\"http://www.irgcis.irri.org:81/grc/TK.exe$Query?DataSource=IRG&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3E%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3C%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID=&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME-OP=ctn&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME=&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY=&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY-OP=%3D&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY=&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE=&GBUSER_TK_MORPH1_2.MAT-OP=%3E%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.MAT-OP=%3C%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRWD-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.GRWD-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.VG-OP=%3D&GBUSER_TK_MORPH1_2.VG=&GBUSER_TK_MORPH1_2.ENDO-OP=%3D&GBUSER_TK_MORPH1_2.ENDO=&GBUSER_TK_MORPH1_2.SCCO_REV-OP=%3D&GBUSER_TK_MORPH1_2.SCCO_REV=&GBUSER_TK_EVAL.BL_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BL_DESCRIPTION=&GBUSER_TK_EVAL.BB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BB_DESCRIPTION=&GBUSER_TK_EVAL.SHB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.SHB_DESCRIPTION=&Output=%2FGRC%2FMorphVegAllSpp.htm&Limit=-1\")\n",
    "morphoreproinfo=requests.get(\"http://www.irgcis.irri.org:81/grc/TK.exe$Query?DataSource=IRG&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3E%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3C%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID=&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME-OP=ctn&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME=&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY=&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY-OP=%3D&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY=&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE=&GBUSER_TK_MORPH1_2.MAT-OP=%3E%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.MAT-OP=%3C%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRWD-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.GRWD-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.VG-OP=%3D&GBUSER_TK_MORPH1_2.VG=&GBUSER_TK_MORPH1_2.ENDO-OP=%3D&GBUSER_TK_MORPH1_2.ENDO=&GBUSER_TK_MORPH1_2.SCCO_REV-OP=%3D&GBUSER_TK_MORPH1_2.SCCO_REV=&GBUSER_TK_EVAL.BL_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BL_DESCRIPTION=&GBUSER_TK_EVAL.BB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BB_DESCRIPTION=&GBUSER_TK_EVAL.SHB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.SHB_DESCRIPTION=&Output=%2FGRC%2FMorpReproAllSpp.htm&Limit=-1\")\n",
    "morphoharvestinfo=requests.get(\"http://www.irgcis.irri.org:81/grc/TK.exe$Query?DataSource=IRG&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3E%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3C%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID=&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME-OP=ctn&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME=&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY=&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY-OP=%3D&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY=&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE=&GBUSER_TK_MORPH1_2.MAT-OP=%3E%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.MAT-OP=%3C%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRWD-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.GRWD-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.VG-OP=%3D&GBUSER_TK_MORPH1_2.VG=&GBUSER_TK_MORPH1_2.ENDO-OP=%3D&GBUSER_TK_MORPH1_2.ENDO=&GBUSER_TK_MORPH1_2.SCCO_REV-OP=%3D&GBUSER_TK_MORPH1_2.SCCO_REV=&GBUSER_TK_EVAL.BL_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BL_DESCRIPTION=&GBUSER_TK_EVAL.BB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BB_DESCRIPTION=&GBUSER_TK_EVAL.SHB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.SHB_DESCRIPTION=&Output=%2FGRC%2FMorphHarvPostAllSpp.htm&Limit=-1\")\n",
    "diseasereactioninfo=requests.get(\"http://www.irgcis.irri.org:81/grc/TK.exe$Query?DataSource=IRG&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3E%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3C%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID=&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME-OP=ctn&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME=&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY=&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY-OP=%3D&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY=&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE=&GBUSER_TK_MORPH1_2.MAT-OP=%3E%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.MAT-OP=%3C%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRWD-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.GRWD-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.VG-OP=%3D&GBUSER_TK_MORPH1_2.VG=&GBUSER_TK_MORPH1_2.ENDO-OP=%3D&GBUSER_TK_MORPH1_2.ENDO=&GBUSER_TK_MORPH1_2.SCCO_REV-OP=%3D&GBUSER_TK_MORPH1_2.SCCO_REV=&GBUSER_TK_EVAL.BL_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BL_DESCRIPTION=&GBUSER_TK_EVAL.BB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BB_DESCRIPTION=&GBUSER_TK_EVAL.SHB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.SHB_DESCRIPTION=&Output=%2FGRC%2FDisease_resist.htm&Limit=-1\")\n",
    "envreactioninfo=requests.get(\"http://www.irgcis.irri.org:81/grc/TK.exe$Query?DataSource=IRG&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.STATUS_ACC=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3E%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO-OP=%3C%3D&GBUSER_TK_PASS1_ORICOUNTRY.ACCNO=&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.SPECIES_REID=&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME-OP=ctn&GBUSER_TK_PASS1_ORICOUNTRY.ALL_ACCNO_NAME=&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.ORI_COUNTRY=&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY-OP=%3D&GBUSER_TK_PASS1_SSCOUNTRY.SS_COUNTRY=&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE-OP=%3D&GBUSER_TK_PASS1_ORICOUNTRY.CULT_TYPE=&GBUSER_TK_MORPH1_2.MAT-OP=%3E%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.MAT-OP=%3C%3D&GBUSER_TK_MORPH1_2.MAT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRLT-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRLT=&GBUSER_TK_MORPH1_2.GRWD-OP=%3E%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.GRWD-OP=%3C%3D&GBUSER_TK_MORPH1_2.GRWD=&GBUSER_TK_MORPH1_2.VG-OP=%3D&GBUSER_TK_MORPH1_2.VG=&GBUSER_TK_MORPH1_2.ENDO-OP=%3D&GBUSER_TK_MORPH1_2.ENDO=&GBUSER_TK_MORPH1_2.SCCO_REV-OP=%3D&GBUSER_TK_MORPH1_2.SCCO_REV=&GBUSER_TK_EVAL.BL_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BL_DESCRIPTION=&GBUSER_TK_EVAL.BB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.BB_DESCRIPTION=&GBUSER_TK_EVAL.SHB_DESCRIPTION-OP=ctn&GBUSER_TK_EVAL.SHB_DESCRIPTION=&Output=%2FGRC%2FPhysiochem.htm&Limit=-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    CPU times: user 40.4 s, sys: 19.3 s, total: 59.7 s\n",
    "    Wall time: 2h 38min 27s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring the Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "dict_builder\n",
    "\n",
    "Inputs\n",
    "------\n",
    "query: the type request from the IRGCIS database\n",
    "    e.g. querying the 'location information'  \n",
    "rows: the HTML returned from the request\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "\n",
    "a list of dictionaries, each of which corresponds to a rice strain with\n",
    "    information depending upon the query type\n",
    "\n",
    "E.g. for a query on location info:\n",
    "\n",
    "{'strain_id': 109900, \n",
    " 'species_name': 'O. japonica', \n",
    " 'variety_name': 'Sativa',\n",
    " 'province': 'Japan',\n",
    " 'district': 'South',\n",
    " 'town': 'Kyoto', \n",
    " 'village': 'null', \n",
    " 'latitude': '23', \n",
    " 'longitude': '55', \n",
    " 'altitude': '200'}\n",
    "\n",
    "Notes\n",
    "-----\n",
    "Each query type returns a unique set of information contained \n",
    "\"\"\"\n",
    "\n",
    "def dict_builder(query,rows):\n",
    "    \n",
    "    # Define fields to define columns from IRGC database for each query\n",
    "    if query == 'basicinfo':\n",
    "        fields=['strain_id', 'species_name', 'variety_name', 'previous_name', \n",
    "                'pedigree', 'collection_number', 'acc_id_seq_num', 'acc_id_seed_donor_number',\n",
    "                'source_country', 'donor_country', 'acc_date', 'status', 'cultural_type',\n",
    "                'special_traits', 'fao_in_trust', 'multilateral_system']\n",
    "    elif query == 'allinfo':\n",
    "        fields=['strain_id', 'species_name', 'variety_name', 'previous_name', \n",
    "                'pedigree', 'collection_number', 'acc_id_seed_donor_number',\n",
    "                'source_country', 'cultural_type','days_to_mat', 'grain_len',\n",
    "                'grain_wid', 'varietal_group', 'endosperm_type', 'endosperm_color',\n",
    "                'seed_coat_color', 'rice_blast', 'bacterial_blight', 'sheath_blight']\n",
    "    elif query == 'locationinfo':\n",
    "        fields=['strain_id', 'species_name', 'variety_name', 'province',\n",
    "                'district', 'town', 'village', 'latitude', 'longitude', 'altitude']\n",
    "    elif query == 'morphoveginfo':\n",
    "        fields=['strain_id', 'subpopulation', 'species_name', 'variety_name', \n",
    "                'source_country', 'blade_pub', 'blade_color', 'basal_color', 'ligule_shape',\n",
    "                'leaf_texture', 'leaf_angle', 'seedling_height', 'ligule_color', \n",
    "                'collar_color', 'auricle_color']\n",
    "    elif query == 'morphoreproinfo':\n",
    "        fields=['strain_id', 'subpopulation', 'species_name', 'variety_name', \n",
    "                'source_country', 'culm_angle', 'node_color', 'internode_color', \n",
    "                'internode_color', 'culm_strength', 'flat_leaf_angle', 'flag_leaf_angle',\n",
    "                'panicle', 'secondary_branching', 'panicle_exerion', 'panicle_axis',\n",
    "                'awn_presence', 'awn_color', 'apiculus_color', 'stigma_color', 'lemma_color',\n",
    "                'leaf_length', 'leaf_width', 'culm_length', 'culm_num', 'culm_diam', \n",
    "                'days_to_head', 'variety_group_from_morph', 'days_to_flower', 'lingule_pub',\n",
    "                'lemma_color_at_anthesis', 'internode_color','culm_strength_cult']\n",
    "    elif query == 'morphoharvestinfo':\n",
    "        fields=['strain_id', 'species_name', 'variety_name', 'source_country', \n",
    "                'days_to_maturity', 'panicle_shattering', 'leaf_senescence', 'spikelet_fertility',\n",
    "                'panicle_thesability', 'apiculus_color_post_harv', 'lemma_and_palea_color',\n",
    "                'lemma_and_palea_pub', 'seat_coat_color']\n",
    "    elif query == 'diseasereactioninfo':\n",
    "        fields=['strain_id', 'species_name', 'variety_name', 'source_country', \n",
    "                'blast', 'bacterial_blight', 'sheath_blight', 'tungro_virus']\n",
    "    elif query == 'envreactioninfo':\n",
    "        fields=['strain_id', 'species_name', 'variety_name', 'source_country', \n",
    "                'alkali_tolerance', 'cold_tolerance', 'drought_1', 'drought_2',\n",
    "                'drought_3', 'drought_4', 'drought_5', 'drought_6', 'drought_7',\n",
    "                'drought_8', 'drought_9', 'elogation', 'flood_tolerance', \n",
    "                'salt_tolerance', 'zinc_def_1', 'zinc_def_1']\n",
    "    else:\n",
    "        fields =[]\n",
    "\n",
    "    # create list to hold the dict for every strain    \n",
    "    ricestrains=[]\n",
    "    for r in rows:\n",
    "        \n",
    "        # iterate through every cell in a row and get the text inside\n",
    "        d_td=pq(r)('td')\n",
    "        a = [pq(d_td[i]).text() for i in range(0,len(pq(d_td)))]\n",
    "\n",
    "        # create a dict for that strain and append to the growing list\n",
    "        ricedict = dict(zip(fields,a))\n",
    "        ricestrains.append(ricedict)\n",
    "    \n",
    "    return ricestrains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLAIN WHAT THE BELOW DOES AND HOW LONG IT WILL TAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a list of all of the queries\n",
    "scrapedlist = [('basicinfo',basicinfo), ('allinfo',allinfo), \n",
    "              ('locationinfo',locationinfo), ('morphoveginfo',morphoveginfo),\n",
    "              ('morphoreproinfo',morphoreproinfo), ('morphoharvestinfo',morphoharvestinfo), \n",
    "              ('diseasereactioninfo',diseasereactioninfo), ('envreactioninfo',envreactioninfo)]\n",
    "\n",
    "dict_list=[]\n",
    "\n",
    "# iterate through queries creating a list of tuples organized as (query,[dicts for each strain]) \n",
    "for k,v in scrapedlist:\n",
    "    \n",
    "    # cleans the HTML before passing to dict_builder\n",
    "    d_= pq(v.text)\n",
    "    d_rows = pq(d_('tr')[4:])\n",
    "    d_rows = pq(d_rows[:(len(d_rows)-1)])\n",
    "\n",
    "    dict_list.append((k, dict_builder(k,pq(d_rows))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing how many strains we recovered for each query, and saving each file as a pickle along the way, we see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in dict_list:\n",
    "    # Print how many rows each df will have\n",
    "    print k + \" rows: \" + str(len(v))\n",
    "    \n",
    "    # Create a dataframe of each and save as a pickle\n",
    "    a = pd.DataFrame(v)\n",
    "    picklename=('tempdata/'+k)\n",
    "    a.to_pickle(picklename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    basicinfo rows: 131112\n",
    "    allinfo rows: 134842\n",
    "    locationinfo rows: 127128\n",
    "    morphoveginfo rows: 130800\n",
    "    morphoreproinfo rows: 130800\n",
    "    morphoharvestinfo rows: 130800\n",
    "    diseasereactioninfo rows: 127128\n",
    "    envreactioninfo rows: 127128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load all of the files from pickles in tempdata and set index to strain_id\n",
    "basicinfo=pd.read_pickle('tempdata/basicinfo').set_index('strain_id')\n",
    "allinfo=pd.read_pickle('tempdata/allinfo').set_index('strain_id')\n",
    "diseasereactioninfo=pd.read_pickle('tempdata/diseasereactioninfo').set_index('strain_id')\n",
    "envreactioninfo=pd.read_pickle('tempdata/envreactioninfo').set_index('strain_id')\n",
    "locationinfo=pd.read_pickle('tempdata/locationinfo').set_index('strain_id')\n",
    "morphoharvestinfo=pd.read_pickle('tempdata/morphoharvestinfo').set_index('strain_id')\n",
    "morphoreproinfo=pd.read_pickle('tempdata/morphoreproinfo').set_index('strain_id')\n",
    "morphoveginfo=pd.read_pickle('tempdata/morphoveginfo').set_index('strain_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save all of the dataframes into an array\n",
    "dataframes=[basicinfo,allinfo,diseasereactioninfo,envreactioninfo,locationinfo,morphoharvestinfo,morphoreproinfo,morphoveginfo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract all of the ids from allinfo (allinfo has the greatest number of strain ids out of all of the dataframes)\n",
    "full_ids=allinfo.index.tolist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "create_new_dataframe\n",
    "\n",
    "Inputs\n",
    "------\n",
    "ids: an array of ids that will be included in the new dataframe\n",
    "dfs: an array of dataframes that will be combined to form the new dataframe\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "\n",
    "a populated dataframe that includes a row for each of the ids inputed \n",
    "    and a column for each of the columns includedd in each of the inputted dataframes\n",
    "\n",
    "Notes\n",
    "-----\n",
    "This code takes over 3 hours to run with all of the previous dataframes included \n",
    "\"\"\"\n",
    "def create_new_dataframe(ids, dfs):\n",
    "    #create array to store column names\n",
    "    columns=[]\n",
    "    #iterate through every dataframe\n",
    "    for d in dfs:\n",
    "        #add column names from dataframe to columns array\n",
    "        columns.extend(list(d.columns.values))\n",
    "    #determine every unique column in array\n",
    "    columns=np.unique(columns)\n",
    "    #create final dataframe with unique columns\n",
    "    fdf=pd.DataFrame(columns=columns)\n",
    "    #set counter to 0\n",
    "    counter=0\n",
    "    #go through each id\n",
    "    for i in ids:\n",
    "        #increase counter by 1\n",
    "        counter +=1\n",
    "        #create dict to store values for the specific row\n",
    "        row={}\n",
    "        #go through each dataframe\n",
    "        for d in dfs:\n",
    "            #if id value is in the dataframe add data to row as dict\n",
    "            if str(i) in d.index.values:\n",
    "                row.update(d.loc[str(i)].to_dict())\n",
    "        #add row to final dataframe\n",
    "        fdf=fdf.append(pd.DataFrame(row, index=[i]))\n",
    "    #after every id has been iterated through, return final dataframe\n",
    "    return fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_info=create_new_dataframe(full_ids, dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the `all_info` dataframe and have an optional load, for faster restarting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving the dataframe\n",
    "all_info.to_csv('data/all_info_combined.csv', encoding='utf-8')\n",
    "# Optional reloading\n",
    "all_info = pd.read_csv('data/all_info_combined.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print all_info.shape\n",
    "all_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subselecting for AWS Strains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AT SOME POINT WE NEED TO DESCRIBE THE AWS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `index.csv` is a file modified (copying data from multiple sheets onto one) from http://www.gigasciencejournal.com/content/supplementary/2047-217x-3-7-s1.xlsx and converted to a CSV. This `index.csv` serves to link the IRGC number, (referred to prior as `strain_id`) to the IRIS number, which is what the AWS SNP data uses to uniquely identify the strains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_df=pd.read_csv(\"data/index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create array to store IRGC Number in\n",
    "array=[]\n",
    "#go through every row in dataframe\n",
    "for r in index_df.index:\n",
    "    #determine if DNA_Accno_source is a string\n",
    "    if isinstance(index_df.ix[r][\"DNA_Accno_source\"], str):\n",
    "        #if it is, add the numeric part of string to array\n",
    "        array.append(int(index_df.ix[r][\"DNA_Accno_source\"][5:]))\n",
    "    #if DNA_Accno_sourse does not exist, add NaN to array\n",
    "    else:\n",
    "        array.append(np.nan)\n",
    "#add array as a new column in index_df\n",
    "index_df.insert(5, \"IRGC_Number\", array);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create merged dataframe by merging index_df rows and all_info rows that share the same strain id number\n",
    "merged_df_updated=pd.merge(left=index_df,right=all_info, left_on='IRGC_Number', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the all_info dataframe and have an optional load, for faster restarting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving the Dataframe\n",
    "merged_df_updated.to_csv(\"data/merged_dataframe_updated.csv\")\n",
    "# Optional reloading\n",
    "merged_df_updated = pd.read_csv('data/merged_dataframe_updated.csv', index_col=0)\n",
    "print merged_df_updated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA of Phenotypic Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many total columns are in the merged_df\n",
    "print \"Number of uncleaned variables: \" + str(len(merged_df_updated.columns))\n",
    "\n",
    "# Generate a new df to preform EDA on, dropping singular values other than strain_id\n",
    "eda_df = merged_df_updated[list(all_info.columns)]\n",
    "\n",
    "for col in eda_df.columns:\n",
    "    if 'Unnamed' in col:\n",
    "        del eda_df[col]\n",
    "        \n",
    "eda_df = eda_df.drop(['acc_date','acc_id_seed_donor_number','acc_id_seq_num',\n",
    "                      'collection_number','variety_name', 'variety_group_from_morph'],1)\n",
    "\n",
    "print \"Number of usable variables: \" + str(len(eda_df.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fullness = % not NAN\n",
    "fullness=[]\n",
    "# Uniqueness = % unique variables w/in variable type\n",
    "uniqueness=[]\n",
    "\n",
    "# Iterate through columns to see what is full / unique\n",
    "for column in eda_df.columns:\n",
    "    values = []\n",
    "    for value in eda_df[column]:\n",
    "        # Checks for NAN values\n",
    "        if value == value:\n",
    "            values.append(value)\n",
    "    fullness.append((column,((float(len(values)))/(float(len(eda_df[column]))))))\n",
    "    uniqueness.append((column,len(np.unique(values))))\n",
    "    \n",
    "# Sort descending    \n",
    "fullness.sort(key=lambda tup: tup[1],reverse=True)\n",
    "uniqueness.sort(key=lambda tup: tup[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = zip(*fullness)\n",
    "plt.hist(y);\n",
    "plt.title('Histogram of Percent Fullness \\n (Not NaN)')\n",
    "plt.xlabel('Percent Full')\n",
    "plt.ylabel('Number');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = zip(*uniqueness)\n",
    "plt.bar(xrange(0,len(x)),y)\n",
    "plt.title('Bar Graph of Uniqueness Within Variables')\n",
    "plt.xlabel('Variable (Sorted order)')\n",
    "plt.ylabel('Number of Unique Variables \\n (Max = 2347)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.bar(xrange(0,20),y[:20])\n",
    "plt.xticks(xrange(0,20),(x[:20]),rotation='vertical',);\n",
    "plt.subplots_adjust(bottom=0.5)\n",
    "plt.title('Bar Graph of Top 20 Most Unique Variable')\n",
    "plt.xlabel('Variable (Sorted order)')\n",
    "plt.ylabel('Number of Unique Variables \\n (Max = 2347)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(eda_df['days_to_mat'], bins=25);\n",
    "plt.title('Number of Days to Maturity')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of Varieties');\n",
    "plt.axvline(np.mean(eda_df['days_to_mat']), label='Mean Days', color='black',ymax=.95)\n",
    "plt.axvline(np.mean(eda_df['days_to_mat'])-np.std(eda_df['days_to_mat']), \n",
    "            label='Fast Grower Cut Off',color='red',ymax=.95)\n",
    "plt.legend(frameon=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Based Prediction of Days to Maturity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metricsdf = pd.read_csv('data/merged_dataframe_updated.csv')\n",
    "\n",
    "mat_df=metricsdf[['DNA_UNIQUE_ID','Variety Group (Tree)1',\n",
    "                  'days_to_mat','source_country','rice_blast']]\n",
    "print mat_df.shape\n",
    "mat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_df = mat_df[mat_df['days_to_mat'] != 0]\n",
    "\n",
    "days_list = np.array(mat_df['days_to_mat'])\n",
    "\n",
    "print \"count: \" + str(len(days_list))\n",
    "print \"min: \" + str(days_list.min())\n",
    "print \"max: \" + str(days_list.max())\n",
    "print \"mean: \" + str(days_list.mean())\n",
    "print \"std: \" + str(days_list.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "itrain, itest = train_test_split(xrange(mat_df.shape[0]), train_size=0.7)\n",
    "mask=np.ones(mat_df.shape[0], dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)\n",
    "mask.shape, mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mat_df[['days_to_mat']] = mat_df[['days_to_mat']].astype(float)\n",
    "\n",
    "std_scale = StandardScaler().fit(mat_df[mask]['days_to_mat'])\n",
    "mat_df['stdardized_mat'] = std_scale.transform(mat_df['days_to_mat'])\n",
    "\n",
    "# We re-define days_list to only define fast-growers based on the training data\n",
    "days_list = np.array(mat_df[mask]['days_to_mat'])\n",
    "\n",
    "fast_grow = [mat_df['days_to_mat'] < (days_list.mean() - days_list.std())]\n",
    "mat_df['fast'] = fast_grow[0]\n",
    "\n",
    "mat_df.to_csv('data/mat_df.csv')\n",
    "\n",
    "print \"Fast Grow Cut Off: \" + str((days_list.mean() - days_list.std())) + \" Days\"\n",
    "mat_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(mat_df['stdardized_mat'], bins=23);\n",
    "plt.title('Standardized Number of Days to Maturity')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of Varieties');\n",
    "plt.axvline(np.mean(mat_df['stdardized_mat']), label='Mean Days', color='black',ymax=.95)\n",
    "plt.axvline(np.mean(mat_df['stdardized_mat'])-np.std(mat_df['stdardized_mat']), \n",
    "            label='Fast Grower Cut Off',color='red',ymax=.95)\n",
    "plt.legend(frameon=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"% fast: \" + str(sum(mat_df['fast'])/float(len((mat_df['fast']))) * 100)\n",
    "print \"% slow: \" + str((1. -sum(mat_df['fast'])/float(len((mat_df['fast'])))) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can achieve approximately **84.89% accuracy** if we *always* predict the rice to be a slow grower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strain Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variety_days = {}\n",
    "\n",
    "for idx, row in enumerate(mat_df[mask].index):\n",
    "    variety = mat_df[mask].iloc[idx]['Variety Group (Tree)1']\n",
    "    days = mat_df[mask].iloc[idx]['days_to_mat']\n",
    "    if variety not in variety_days.keys():\n",
    "        variety_days.update({variety:[days]})\n",
    "    else:\n",
    "        variety_days.get(variety).append(days)\n",
    "for key in variety_days.keys():\n",
    "    days = np.mean(variety_days.get(key))\n",
    "    variety_days.update({key:days})\n",
    "    \n",
    "variety_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.bar(xrange(0,len(variety_days.values())),variety_days.values());\n",
    "plt.title('Average Days to Maturity by Variety');\n",
    "plt.ylabel('Days');\n",
    "plt.xlabel('Variety Type');\n",
    "plt.axis([0,7,0,150])\n",
    "plt.xticks(xrange(0,len(variety_days.values())),(variety_days.keys()),rotation='45',);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "for row in enumerate(mat_df[~mask].index):\n",
    "    variety = mat_df[~mask].iloc[row[0]]['Variety Group (Tree)1']\n",
    "    pred = (0.5*days_list.mean() + 0.5*variety_days.get(variety))\n",
    "    if pred < (days_list.mean() - days_list.std()):\n",
    "        a.append(True)\n",
    "    else:\n",
    "        a.append(False)\n",
    "    b.append(pred)\n",
    "        \n",
    "conf = confusion_matrix(mat_df[~mask]['fast'], a)\n",
    "        \n",
    "print conf\n",
    "print \"Accuracy: \" + str(float(conf[0][0]+conf[1][1])/float(len(mat_df[~mask]))*100) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutoff = (days_list.mean() - days_list.std())\n",
    "plt.scatter(mat_df[~mask]['days_to_mat'],b);\n",
    "plt.title('Predicted vs. Actual Days to Maturity\\n(Variety-Mean Model)');\n",
    "plt.xlabel('Actual Days');\n",
    "plt.ylabel('Predicted Days');\n",
    "plt.plot([80,220],[80,220],'k--', alpha=0.5, label='45 degree line');\n",
    "plt.axis([80,220,80,220]);\n",
    "plt.axhline(cutoff, color='black',label='Cut Off Value');\n",
    "plt.axvline(cutoff, color='r',label='Actual Fast Growers');\n",
    "plt.legend(frameon=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strain-Country Baseline Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_days = {}\n",
    "\n",
    "for idx, row in enumerate(mat_df[mask].index):\n",
    "    source = mat_df[mask].iloc[idx]['source_country']\n",
    "    days = mat_df[mask].iloc[idx]['days_to_mat']\n",
    "    if source not in source_days.keys():\n",
    "        source_days.update({source:[days]})\n",
    "    else:\n",
    "        source_days.get(source).append(days)\n",
    "for key in source_days.keys():\n",
    "    days = np.mean(source_days.get(key))\n",
    "    num = len(source_days.get(key))\n",
    "    source_days.update({key:(days,num)})\n",
    "    \n",
    "print \"Country: \" + str(source_days.keys()[2])\n",
    "print \"Avg. Days: \" + str(source_days.get(source_days.keys()[2])[0])\n",
    "print \"Strains from Country: \" + str(source_days.get(source_days.keys()[2])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = (np.sort([source_days.get(source)[1] for source in source_days]))\n",
    "plt.bar(xrange(0,len(a)),a);\n",
    "plt.title('Number of Strain by Source Country');\n",
    "plt.ylabel('Number of Strains');\n",
    "plt.xlabel('Source Country');\n",
    "plt.xticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.sort([source_days.get(source)[0] for source in source_days])\n",
    "plt.bar(xrange(0,len(a)),a);\n",
    "plt.title('Average Days to Maturity by Country');\n",
    "plt.ylabel('Days');\n",
    "plt.xlabel('Source Country');\n",
    "plt.xticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "b = []\n",
    "print (days_list.mean() - days_list.std())\n",
    "for row in enumerate(mat_df[~mask].index):\n",
    "    variety = mat_df[~mask].iloc[row[0]]['Variety Group (Tree)1']\n",
    "    source = mat_df[~mask].iloc[row[0]]['source_country']\n",
    "    if source in source_days.keys():\n",
    "        pred = (source_days.get(source)[0])\n",
    "    else:\n",
    "        pred = (days_list.mean()+variety_days.get(variety))/2\n",
    "        \n",
    "    if pred < (days_list.mean() - days_list.std()):\n",
    "        a.append(True)\n",
    "    else:\n",
    "        a.append(False)\n",
    "    b.append(pred)\n",
    "conf = confusion_matrix(mat_df[~mask]['fast'], a)\n",
    "    \n",
    "print conf\n",
    "print \"Accuracy: \" + str(float(conf[0][0]+conf[1][1])/float(len(mat_df[~mask]))*100) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(mat_df[~mask]['days_to_mat'],b);\n",
    "plt.title('Predicted vs. Actual Days to Maturity\\n(Source OR Variety-Mean Model)');\n",
    "plt.xlabel('Actual Days');\n",
    "plt.ylabel('Predicted Days');\n",
    "plt.plot([80,220],[80,220],'k--', alpha=0.5, label='45 degree line');\n",
    "plt.axis([80,220,80,220]);\n",
    "plt.axhline(cutoff, color='black',label='Cut Off Value');\n",
    "plt.axvline(cutoff, color='r',label='Actual Fast Growers');\n",
    "plt.legend(frameon=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = mat_df[mask]\n",
    "\n",
    "train_var_days = []\n",
    "train_source_days = []\n",
    "for row,_ in enumerate(train_df.index):\n",
    "    var = train_df.iloc[row]['Variety Group (Tree)1']\n",
    "    source = train_df.iloc[row]['source_country']\n",
    "    if source in source_days.keys():\n",
    "        train_source_days.append((source_days.get(source)[0]))\n",
    "    else:\n",
    "        train_source_days.append(days_list.mean())\n",
    "    train_var_days.append((variety_days.get(var)))\n",
    "    \n",
    "train_df.loc[:,'source_val'] = train_source_days\n",
    "train_df.loc[:,'var_val'] = train_var_days\n",
    "train_df.loc[:,'mean'] = days_list.mean()\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm, ols\n",
    "\n",
    "ols_model = ols('days_to_mat ~ source_val + var_val + mean', train_df).fit()\n",
    "ols_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "b = []\n",
    "mean = days_list.mean()\n",
    "\n",
    "for row in enumerate(mat_df[~mask].index):\n",
    "    variety = mat_df[~mask].iloc[row[0]]['Variety Group (Tree)1']\n",
    "    source = mat_df[~mask].iloc[row[0]]['source_country']\n",
    "    \n",
    "    if source in source_days.keys():\n",
    "        pred = (ols_model.params.Intercept + ols_model.params.source_val*source_days.get(source)[0]\n",
    "        + ols_model.params.var_val*variety_days.get(variety) + ols_model.params['mean']*mean)\n",
    "   \n",
    "    else:\n",
    "        pred = (ols_model.params.Intercept + ols_model.params.source_val*mean\n",
    "        + ols_model.params.var_val*variety_days.get(variety) + ols_model.params['mean']*mean)\n",
    "\n",
    "    if pred < (days_list.mean() - days_list.std()):\n",
    "        a.append(True)\n",
    "    else:\n",
    "        a.append(False)\n",
    "    b.append(pred)\n",
    "conf = confusion_matrix(mat_df[~mask]['fast'], a)\n",
    "    \n",
    "print conf\n",
    "print \"Accuracy: \" + str(float(conf[0][0]+conf[1][1])/float(len(mat_df[~mask]))*100) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(mat_df[~mask]['days_to_mat'],b);\n",
    "plt.title('Predicted vs. Actual Days to Maturity\\n(Source-Variety-Mean Regression Model)');\n",
    "plt.xlabel('Actual Days');\n",
    "plt.ylabel('Predicted Days');\n",
    "plt.plot([80,220],[80,220],'k--', alpha=0.5, label='45 degree line');\n",
    "plt.axis([80,220,80,220]);\n",
    "plt.axhline(cutoff, color='black',label='Cut Off Value');\n",
    "plt.axvline(cutoff, color='r',label='Actual Fast Growers');\n",
    "plt.legend(frameon=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Strain-Country Baseline Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stddays_list = [item for item in mat_df[mask]['stdardized_mat']]\n",
    "stdmean = np.mean(stddays_list)\n",
    "\n",
    "stdvariety_days = {}\n",
    "\n",
    "for idx, row in enumerate(mat_df[mask].index):\n",
    "    variety = mat_df[mask].iloc[idx]['Variety Group (Tree)1']\n",
    "    days = mat_df[mask].iloc[idx]['stdardized_mat']\n",
    "    if variety not in stdvariety_days.keys():\n",
    "        stdvariety_days.update({variety:[days]})\n",
    "    else:\n",
    "        stdvariety_days.get(variety).append(days)\n",
    "for key in stdvariety_days.keys():\n",
    "    days = np.mean(stdvariety_days.get(key))\n",
    "    stdvariety_days.update({key:days})\n",
    "    \n",
    "stdsource_days = {}\n",
    "\n",
    "for idx, row in enumerate(mat_df[mask].index):\n",
    "    source = mat_df[mask].iloc[idx]['source_country']\n",
    "    days = mat_df[mask].iloc[idx]['stdardized_mat']\n",
    "    if source not in stdsource_days.keys():\n",
    "        stdsource_days.update({source:[days]})\n",
    "    else:\n",
    "        stdsource_days.get(source).append(days)\n",
    "for key in stdsource_days.keys():\n",
    "    days = np.mean(stdsource_days.get(key))\n",
    "    num = len(stdsource_days.get(key))\n",
    "    stdsource_days.update({key:(days,num)})\n",
    "    \n",
    "stdtrain_var_days = []\n",
    "stdtrain_source_days = []\n",
    "for row,_ in enumerate(train_df.index):\n",
    "    var = train_df.iloc[row]['Variety Group (Tree)1']\n",
    "    source = train_df.iloc[row]['source_country']\n",
    "    if source in stdsource_days.keys():\n",
    "        stdtrain_source_days.append((stdsource_days.get(source)[0]))\n",
    "    else:\n",
    "        stdtrain_source_days.append(0)\n",
    "    stdtrain_var_days.append((stdvariety_days.get(var)))\n",
    "    \n",
    "train_df.loc[:,'stdsource_val'] = (stdtrain_source_days)\n",
    "train_df.loc[:,'stdvar_val'] = (stdtrain_var_days)\n",
    "train_df.loc[:,'mean'] = days_list.mean()\n",
    "train_df.loc[:,'std_mean'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.sort([stdsource_days.get(source)[0] for source in stdsource_days])\n",
    "plt.bar(xrange(0,len(a)),a, alpha=.75);\n",
    "plt.title('Standardized Days to Maturity by Country');\n",
    "plt.ylabel('Value');\n",
    "plt.xlabel('Source Country');\n",
    "plt.xticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stdols_model = ols('stdardized_mat ~ stdsource_val + stdvar_val + std_mean', train_df).fit()\n",
    "stdols_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "b=[]\n",
    "\n",
    "for row in enumerate(mat_df[~mask].index):\n",
    "    variety = mat_df[~mask].iloc[row[0]]['Variety Group (Tree)1']\n",
    "    source = mat_df[~mask].iloc[row[0]]['source_country']\n",
    "    \n",
    "    if source in stdsource_days.keys():\n",
    "        pred = (stdols_model.params.Intercept + stdols_model.params.stdsource_val*stdsource_days.get(source)[0]\n",
    "        + stdols_model.params.stdvar_val*stdvariety_days.get(variety))\n",
    "   \n",
    "    else:\n",
    "        pred = (stdols_model.params.Intercept + stdols_model.params.stdvar_val*stdvariety_days.get(variety))\n",
    "\n",
    "    if pred < (0 - np.std(stddays_list)):\n",
    "        a.append(True)\n",
    "    else:\n",
    "        a.append(False)\n",
    "    b.append(pred)\n",
    "conf = confusion_matrix(mat_df[~mask]['fast'], a)\n",
    "    \n",
    "print conf\n",
    "print \"Accuracy: \" + str(float(conf[0][0]+conf[1][1])/float(len(mat_df[~mask]))*100) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(mat_df[~mask]['stdardized_mat'],b);\n",
    "plt.title('Predicted vs. Actual Days to Maturity\\n(Standardized Source-Variety-Mean Regression Model)');\n",
    "plt.xlabel('Actual Days');\n",
    "plt.ylabel('Predicted Days');\n",
    "plt.plot([-3,3],[-3,3],'k--', alpha=0.5, label='45 degree line');\n",
    "plt.axis([-3,3,-3,3]);\n",
    "plt.axhline(-1, color='black',label='Cut Off Value');\n",
    "plt.axvline(-1, color='r',label='Actual Fast Growers');\n",
    "plt.legend(frameon=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing SNP Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.broadinstitute.org/files/news/stories/full/SNP_260x260.jpg\" width=200 height=300/>\n",
    "Image from: https://www.broadinstitute.org/files/news/stories/full/SNP_260x260.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SNP data was downwloaded as PED and MAP files from https://s3.amazonaws.com/3kricegenome/reduced/NB-core_v4.ped.gz and https://s3.amazonaws.com/3kricegenome/reduced/NB-core_v4.map.gz. These files were then converted to a VCF file using plink 1.9 (can be downloaded at https://www.cog-genomics.org/plink2) with the terminal command `command: ./plink --file tempdata/NB-core_v4 --recode vcf`. This was done to make the data in a format compatible with the PyVCF package. The VCF file was over 12 GB, so we could not include it in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vcf_reader=vcf.Reader(open('tempdata/plink.vcf', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create counter\n",
    "i = 0\n",
    "#create dataframe to hold snp data with samples as columns\n",
    "snpdf=pd.DataFrame(columns=vcf_reader.samples)\n",
    "#create default dict list to hold snps\n",
    "snps = defaultdict(list)\n",
    "#create array to hold index values\n",
    "ind_array=[]\n",
    "#iterate through every snp\n",
    "for snp in vcf_reader:\n",
    "    #create a location string containing chromosome and position\n",
    "    loc=(str(snp.CHROM)+'_'+str(snp.POS))\n",
    "    #add loc string to ind_array\n",
    "    ind_array.append(loc)\n",
    "    #increase counter\n",
    "    i = i+1\n",
    "    #go through every sample for snp\n",
    "    for sample in (snp.samples):\n",
    "        #add sample name as key and genotype as value to snps dict\n",
    "        snps[sample.sample].append(sample.data.GT)\n",
    "    if (i % 1000) == 0:\n",
    "        #create dataframe from snps dict\n",
    "        df=pd.DataFrame.from_dict(snps)\n",
    "        #set index column of dataframe to be ind_array\n",
    "        df['index']=ind_array\n",
    "        #set index to index column\n",
    "        df=df.set_index('index')\n",
    "        #remove name of index\n",
    "        df.index.name = None\n",
    "        #add dataframe to full snp dataframe\n",
    "        snpdf=snpdf.append(df)\n",
    "        #reset snps dict\n",
    "        snps={}\n",
    "        snps = defaultdict(list)\n",
    "        #reset ind_array\n",
    "        ind_array=[]\n",
    "    if (i%50000)==0:\n",
    "        #create csv of 50,000 snps\n",
    "        name=('tempdata/snps'+str(i/50000)+'.csv')\n",
    "        snpdf.to_csv(name)\n",
    "        #reset dataframe\n",
    "        snpdf=pd.DataFrame(columns=vcf_reader.samples)\n",
    "        print 'created' + str(name)\n",
    "#save final snp data to csv\n",
    "snpdf.to_csv('tempdata/snps20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CPU times: user 6h 56min 47s, sys: 11min 6s, total: 7h 7min 53s, Wall time: 7h 9min 33s`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 20 CSV files created from the VCF file abover were each over 500 MB in size, so we were unable to include them in our repository. However, they can be downloaded within a compressed file from this Dropbox link: https://www.dropbox.com/sh/orgk51k8pgka69p/AABRBRvb9gDQgRtcuKKLYVnZa?dl=0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### EDA of SNP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snpedadf= pd.read_csv('tempdata/snps1.csv', index_col=0)\n",
    "print snpedadf.shape\n",
    "snpedadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snpedadf = snpedadf.replace(['0/0', '0/1', '1/1', np.nan], [0, 1, 2, 3])\n",
    "snpedadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_uniques = []\n",
    "for row in snpedadf.iterrows():\n",
    "    index_uniques.append(np.bincount(row[1].values))\n",
    "index_uniques[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pltter=[]\n",
    "for index in index_uniques:\n",
    "    if len(index) == 4:\n",
    "          pltter.append(index)\n",
    "            \n",
    "a = [float(arr[0]) for arr in pltter]\n",
    "b = [float(arr[1]) for arr in pltter]\n",
    "c = [float(arr[2]) for arr in pltter]\n",
    "d = [float(arr[3]) for arr in pltter]\n",
    "count = float((len(snpedadf.columns)*len(snpedadf.index)))\n",
    "\n",
    "pcents = [(np.sum(a)/count),(np.sum(b)/count),(np.sum(c)/count),(np.sum(d)/count)]\n",
    "plt.bar(xrange(0,4),pcents);\n",
    "plt.title('% of SNPs of Types');\n",
    "plt.ylabel('Percent');\n",
    "plt.xticks([.4,1.4,2.4,3.4],['Homozygous\\nReference','Heterozygous','Homozygous\\nMutant','NaN'],rotation='0',);\n",
    "print \"NaN: \" + str((np.sum(d)/count)*100) + \" %\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = [float(arr[0])/float(len(snpedadf.columns)) for arr in pltter]\n",
    "b = [float(arr[1])/float(len(snpedadf.columns)) for arr in pltter]\n",
    "c = [float(arr[2])/float(len(snpedadf.columns)) for arr in pltter]\n",
    "d = [float(arr[3])/float(len(snpedadf.columns)) for arr in pltter]\n",
    "\n",
    "f, axarr = plt.subplots(2, 2)\n",
    "\n",
    "axarr[0, 0].hist(a,alpha=1);\n",
    "axarr[0, 0].set_title('Homozygous, Reference',fontsize=14);\n",
    "axarr[0, 0].set_xlim([0, 1])\n",
    "axarr[0, 0].set_ylim([0, 40000])\n",
    "\n",
    "axarr[0, 1].hist(b,alpha=1);\n",
    "axarr[0, 1].set_title('Heterozygous',fontsize=14);\n",
    "axarr[0, 1].set_xlim([0, 1])\n",
    "axarr[0, 1].set_ylim([0, 40000])\n",
    "\n",
    "axarr[1, 0].hist(c,alpha=1,);\n",
    "axarr[1, 0].set_title('Homozygous, Mutant',fontsize=14);\n",
    "axarr[1, 0].set_xlim([0, 1])\n",
    "axarr[1, 0].set_ylim([0, 40000])\n",
    "\n",
    "axarr[1, 1].hist(d,alpha=1);\n",
    "axarr[1, 1].set_title('NaN',fontsize=14);\n",
    "axarr[1, 1].set_xlim([0, 1])\n",
    "axarr[1, 1].set_ylim([0, 40000])\n",
    "\n",
    "f.suptitle('Percent of SNP Type \\n by Individual SNP',fontsize=20);\n",
    "plt.legend(frameon=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snp_edadf= pd.read_csv('tempdata/finally1.csv', index_col=0)\n",
    "print snp_edadf.shape\n",
    "snp_edadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "snp_cols = list(snp_edadf.columns[1:])\n",
    "\n",
    "spearman_cor_dict = {}\n",
    "p_val_dict = {}\n",
    "neg_log_p_val_dict = {}\n",
    "for snp in snp_cols:\n",
    "    cor,p = spearmanr(list(snp_edadf['fast']),list(snp_edadf[snp]))\n",
    "    spearman_cor_dict.update({snp:(cor)}) \n",
    "    p_val_dict.update({snp:(p)})\n",
    "    pp = -np.log10(p)\n",
    "    neg_log_p_val_dict.update({snp:(pp)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats_df=pd.DataFrame(spearman_cor_dict, index=['spearman_cor']).append(pd.DataFrame(p_val_dict, index=['p_vals'])).append(pd.DataFrame(neg_log_p_val_dict, index=['neg_log_p_vals']))\n",
    "stats_df=stats_df.transpose()\n",
    "\n",
    "new_names = []\n",
    "for name in stats_df.index.values:\n",
    "    i=0\n",
    "    for c in name:\n",
    "        i+=1\n",
    "        if c=='_':\n",
    "            upd_name=int(name[i:])\n",
    "            break\n",
    "    new_names.append(upd_name)\n",
    "\n",
    "stats_df.loc[:,'name']=(new_names)\n",
    "stats_df.set_index('name',inplace=True)\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(stats_df.index.values,stats_df['neg_log_p_vals'],alpha=.15)\n",
    "plt.axis([0,np.max(stats_df.index.values),0,60]);\n",
    "plt.title('Manhattan Plot of First 50k SNPs\\n(Correlation to Fast Growth)');\n",
    "plt.ylabel('-log10 (p-value)')\n",
    "plt.xlabel('SNP Location');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(stats_df.index.values,stats_df['spearman_cor'],alpha=.15);\n",
    "plt.axis([0,np.max(stats_df.index.values),-.4,.4]);\n",
    "plt.title('Spearman Rho Values \\n(Correlation to Fast Growth)');\n",
    "plt.ylabel('Rho Value')\n",
    "plt.xlabel('SNP Location');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Predicting Days to Maturity with SNP Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs=['../SNP_Data/snps1.csv','../SNP_Data/snps2.csv','../SNP_Data/snps3.csv',\n",
    "     '../SNP_Data/snps4.csv','../SNP_Data/snps5.csv','../SNP_Data/snps6.csv',\n",
    "     '../SNP_Data/snps7.csv','../SNP_Data/snps8.csv','../SNP_Data/snps9.csv',\n",
    "     '../SNP_Data/snps10.csv','../SNP_Data/snps11.csv','../SNP_Data/snps12.csv',\n",
    "     '../SNP_Data/snps13.csv','../SNP_Data/snps14.csv','../SNP_Data/snps15.csv',\n",
    "     '../SNP_Data/snps16.csv','../SNP_Data/snps17.csv','../SNP_Data/snps18.csv',\n",
    "     '../SNP_Data/snps19.csv','../SNP_Data/snps20.csv',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_useable(dfs,merged_df):\n",
    "    counter=0\n",
    "    for df_name in dfs:\n",
    "        counter+=1\n",
    "        snpdf=pd.read_csv(df_name, index_col=0)\n",
    "        \n",
    "        snpdf=snpdf.transpose()\n",
    "        \n",
    "        select  = list(snpdf.index)\n",
    "        ARRAY=[]\n",
    "        for name in select:\n",
    "            if name[:4]=='IRIS':\n",
    "                und=0\n",
    "                i=0\n",
    "                for c in name:\n",
    "                    if c=='_':\n",
    "                        und+=1\n",
    "                    if und==2:\n",
    "                        upd_name=name[:i]\n",
    "                        upd_name=upd_name.replace('_', ' ')\n",
    "                        if upd_name in list(metricsdf.DNA_UNIQUE_ID):\n",
    "                            ARRAY.append(upd_name)\n",
    "                        else:\n",
    "                            ARRAY.append(name)\n",
    "                        break\n",
    "                    i+=1\n",
    "            else:\n",
    "                ARRAY.append(name)\n",
    "                \n",
    "        snpdf['index']=ARRAY\n",
    "        \n",
    "        snpdf.set_index('index', inplace=True)\n",
    "        \n",
    "        snpdf.index.name = None\n",
    "        \n",
    "        snpdf=snpdf.transpose()\n",
    "        \n",
    "        snpdf = snpdf.replace(['0/0', '0/1', '1/1', np.nan], [0, 1, 2, 0])\n",
    "        \n",
    "        snpdf=snpdf.transpose()\n",
    "        \n",
    "        merged_df=pd.merge(left=metricsdf,right=snpdf, left_on='DNA_UNIQUE_ID', right_index=True)\n",
    "        \n",
    "        merged_df.set_index('DNA_UNIQUE_ID', inplace=True)\n",
    "        \n",
    "        merged_df.drop(['Variety Group (Tree)1', 'source_country', 'rice_blast', 'days_to_mat','stdardized_mat','z_score'], axis=1,inplace=True)\n",
    "        \n",
    "        merged_df[['fast']] = merged_df[['fast']].astype(int)\n",
    "        \n",
    "        print ('making csv'+str(counter))\n",
    "        \n",
    "        merged_df.to_csv('../finally'+str(counter)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_useable(dfs,mat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "cv_optimize\n",
    "\n",
    "Inputs\n",
    "------\n",
    "clf : an instance of a scikit-learn classifier\n",
    "parameters: a parameter grid dictionary thats passed to GridSearchCV (see above)\n",
    "X: a samples-features matrix in the scikit-learn style\n",
    "y: the response vectors of 1s and 0s (+ives and -ives)\n",
    "n_folds: the number of cross-validation folds (default 5)\n",
    "score_func: a score function we might want to pass (default python None)\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "The best estimator from the GridSearchCV, after the GridSearchCV has been used to\n",
    "fit the model.\n",
    "     \n",
    "Notes\n",
    "-----\n",
    "see do_classify and the code below for an example of how this is used\n",
    "\"\"\"\n",
    "#your code here\n",
    "def cv_optimize(clf, parameters, X, y, n_folds=5, score_func=None):\n",
    "    #create gridsearch\n",
    "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=score_func)\n",
    "    #fit gridsearch to inputed values\n",
    "    gs.fit(X, y)\n",
    "    #determine best estimator and return it\n",
    "    best = gs.best_estimator_\n",
    "    return best\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, mask=None, reuse_split=None, score_func=None, n_folds=5):\n",
    "    subdf=indf[featurenames]\n",
    "    X=subdf.values\n",
    "    y=(indf[targetname].values==target1val)*1\n",
    "    if mask !=None:\n",
    "        print \"using mask\"\n",
    "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
    "    if reuse_split !=None:\n",
    "        print \"using reuse split\"\n",
    "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n",
    "    if parameters:\n",
    "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print \"############# based on standard predict ################\"\n",
    "    print \"Accuracy on training data: %0.4f\" % (training_accuracy)\n",
    "    print \"Accuracy on test data:     %0.4f\" % (test_accuracy)\n",
    "    print confusion_matrix(ytest, clf.predict(Xtest))\n",
    "    print \"########################################################\"\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest\n",
    "\n",
    "def pearson_scorer(X,y):\n",
    "    rs=np.zeros(X.shape[1])\n",
    "    pvals=np.zeros(X.shape[1])\n",
    "    i=0\n",
    "    for v in X.T:\n",
    "        rs[i], pvals[i]=pearsonr(v, y)\n",
    "        i=i+1\n",
    "    return np.abs(rs), pvals \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.svm import LinearSVC\n",
    "import operator\n",
    "from sklearn import feature_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_analysis(df_names):\n",
    "    counter=0\n",
    "    for df_name in df_names:\n",
    "        counter+=1\n",
    "        \n",
    "        print ('snps '+str(counter))\n",
    "        \n",
    "        merged_df=pd.read_csv(df_name, index_col=0)\n",
    "        \n",
    "        lcols=list(merged_df)\n",
    "        lcols.remove('fast')\n",
    "        \n",
    "        itrain, itest = train_test_split(xrange(merged_df.shape[0]), train_size=0.7)\n",
    "        mask=np.ones(merged_df.shape[0], dtype='int')\n",
    "        mask[itrain]=1\n",
    "        mask[itest]=0\n",
    "        mask = (mask==1)\n",
    "        mask.shape, mask.sum()\n",
    "        \n",
    "        clflog, Xtrain, ytrain, Xtest, ytest = do_classify(LogisticRegression(penalty=\"l1\"), {\"C\": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, merged_df,lcols, 'fast',1, mask=mask)\n",
    "        \n",
    "        clfsvm_final, Xtrain, ytrain, Xtest, ytest = do_classify(LinearSVC(loss=\"hinge\"), {\"C\": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, merged_df,lcols, 'fast',1, mask=mask)\n",
    "        \n",
    "        reuse_split=dict(Xtrain=Xtrain, Xtest=Xtest, ytrain=ytrain, ytest=ytest)\n",
    "        \n",
    "        clfsvm=LinearSVC(loss=\"hinge\")\n",
    "        Cs=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "        Xmatrix=merged_df[lcols].values\n",
    "        Yresp=merged_df['fast'].values\n",
    "\n",
    "        Xmatrix_train=Xmatrix[mask]\n",
    "        Xmatrix_test=Xmatrix[~mask]\n",
    "        Yresp_train=Yresp[mask]\n",
    "        Yresp_test=Yresp[~mask]\n",
    "\n",
    "        #set parameters\n",
    "        parameters = {\"C\": Cs}\n",
    "        #create fitmodel with classifier\n",
    "        fitmodel = GridSearchCV(clfsvm, param_grid=parameters, cv=5, scoring=\"accuracy\")\n",
    "        #fit model with training data\n",
    "        fitmodel.fit(Xmatrix_train, Yresp_train)\n",
    "        #output best values\n",
    "        print fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_\n",
    "        \n",
    "        selectorlinearsvm = SelectKBest(k=25, score_func=pearson_scorer)\n",
    "        pipelinearsvm = Pipeline([('select', selectorlinearsvm), ('svm', LinearSVC(loss=\"hinge\"))])\n",
    "        \n",
    "        pipelinearsvm, _,_,_,_  = do_classify(pipelinearsvm, {\"svm__C\": [0.00001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, merged_df,lcols, 'fast',1, reuse_split=reuse_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upd_dfs=['../finally1.csv','../finally2.csv','../finally3.csv',\n",
    "     '../finally4.csv','../finally5.csv','../finally6.csv',\n",
    "     '../finally7.csv','../finally8.csv','../finally9.csv',\n",
    "     '../finally10.csv','../finally11.csv','../finally12.csv',\n",
    "     '../finally13.csv','../finally14.csv','../finally15.csv',\n",
    "     '../finally16.csv','../finally17.csv','../finally18.csv',\n",
    "     '../finally19.csv','../finally20.csv',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_analysis(upd_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    snps 1\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.86\n",
    "    [[602   0]\n",
    "     [101   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.90\n",
    "    [[577  25]\n",
    "     [ 47  54]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.001} 0.799390243902 [mean: 0.79939, std: 0.05104, params: {'C': 0.001}, mean: 0.79817, std: 0.04958, params: {'C': 0.01}, mean: 0.79817, std: 0.04958, params: {'C': 0.1}, mean: 0.79817, std: 0.04958, params: {'C': 1.0}, mean: 0.79817, std: 0.04958, params: {'C': 10.0}, mean: 0.79817, std: 0.04958, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.83\n",
    "    Accuracy on test data:     0.85\n",
    "    [[548  54]\n",
    "     [ 49  52]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 2\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[595   0]\n",
    "     [108   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.86\n",
    "    [[560  35]\n",
    "     [ 64  44]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.01} 0.796341463415 [mean: 0.79573, std: 0.05726, params: {'C': 0.001}, mean: 0.79634, std: 0.05800, params: {'C': 0.01}, mean: 0.79634, std: 0.05800, params: {'C': 0.1}, mean: 0.79634, std: 0.05800, params: {'C': 1.0}, mean: 0.79634, std: 0.05800, params: {'C': 10.0}, mean: 0.79634, std: 0.05800, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[595   0]\n",
    "     [108   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 3\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.84\n",
    "    [[592   0]\n",
    "     [111   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.86\n",
    "    [[557  35]\n",
    "     [ 62  49]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.001} 0.806097560976 [mean: 0.80610, std: 0.06609, params: {'C': 0.001}, mean: 0.80488, std: 0.06607, params: {'C': 0.01}, mean: 0.80488, std: 0.06607, params: {'C': 0.1}, mean: 0.80488, std: 0.06607, params: {'C': 1.0}, mean: 0.80488, std: 0.06607, params: {'C': 10.0}, mean: 0.80488, std: 0.06607, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.84\n",
    "    [[592   0]\n",
    "     [111   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 4\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[597   0]\n",
    "     [106   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.86\n",
    "    [[557  40]\n",
    "     [ 58  48]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.01} 0.818292682927 [mean: 0.81585, std: 0.05092, params: {'C': 0.001}, mean: 0.81829, std: 0.05347, params: {'C': 0.01}, mean: 0.81829, std: 0.05347, params: {'C': 0.1}, mean: 0.81829, std: 0.05347, params: {'C': 1.0}, mean: 0.81829, std: 0.05347, params: {'C': 10.0}, mean: 0.81829, std: 0.05347, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[597   0]\n",
    "     [106   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 5\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.86\n",
    "    Accuracy on test data:     0.83\n",
    "    [[585   0]\n",
    "     [118   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.85\n",
    "    [[552  33]\n",
    "     [ 73  45]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.001} 0.82256097561 [mean: 0.82256, std: 0.05687, params: {'C': 0.001}, mean: 0.81463, std: 0.05583, params: {'C': 0.01}, mean: 0.81463, std: 0.05583, params: {'C': 0.1}, mean: 0.81463, std: 0.05583, params: {'C': 1.0}, mean: 0.81463, std: 0.05583, params: {'C': 10.0}, mean: 0.81463, std: 0.05583, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.88\n",
    "    Accuracy on test data:     0.83\n",
    "    [[566  19]\n",
    "     [ 98  20]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 6\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[598   0]\n",
    "     [105   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.87\n",
    "    [[573  25]\n",
    "     [ 68  37]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.001} 0.782317073171 [mean: 0.78232, std: 0.06552, params: {'C': 0.001}, mean: 0.78110, std: 0.06824, params: {'C': 0.01}, mean: 0.78110, std: 0.06824, params: {'C': 0.1}, mean: 0.78110, std: 0.06824, params: {'C': 1.0}, mean: 0.78110, std: 0.06824, params: {'C': 10.0}, mean: 0.78110, std: 0.06824, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[598   0]\n",
    "     [105   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 7\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.84\n",
    "    [[590   0]\n",
    "     [113   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.86\n",
    "    [[559  31]\n",
    "     [ 65  48]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.01} 0.816463414634 [mean: 0.81585, std: 0.04601, params: {'C': 0.001}, mean: 0.81646, std: 0.04828, params: {'C': 0.01}, mean: 0.81646, std: 0.04828, params: {'C': 0.1}, mean: 0.81646, std: 0.04828, params: {'C': 1.0}, mean: 0.81646, std: 0.04828, params: {'C': 10.0}, mean: 0.81646, std: 0.04828, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.84\n",
    "    [[590   0]\n",
    "     [113   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 8\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.84\n",
    "    Accuracy on test data:     0.87\n",
    "    [[610   0]\n",
    "     [ 93   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.87\n",
    "    [[580  30]\n",
    "     [ 61  32]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.001} 0.796341463415 [mean: 0.79634, std: 0.05843, params: {'C': 0.001}, mean: 0.79390, std: 0.05730, params: {'C': 0.01}, mean: 0.79390, std: 0.05730, params: {'C': 0.1}, mean: 0.79390, std: 0.05730, params: {'C': 1.0}, mean: 0.79390, std: 0.05730, params: {'C': 10.0}, mean: 0.79390, std: 0.05730, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.84\n",
    "    Accuracy on test data:     0.87\n",
    "    [[610   0]\n",
    "     [ 93   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 9\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.86\n",
    "    [[602   0]\n",
    "     [101   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.89\n",
    "    [[571  31]\n",
    "     [ 48  53]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.01} 0.795731707317 [mean: 0.79207, std: 0.05024, params: {'C': 0.001}, mean: 0.79573, std: 0.05006, params: {'C': 0.01}, mean: 0.79573, std: 0.05006, params: {'C': 0.1}, mean: 0.79573, std: 0.05006, params: {'C': 1.0}, mean: 0.79573, std: 0.05006, params: {'C': 10.0}, mean: 0.79573, std: 0.05006, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.86\n",
    "    [[602   0]\n",
    "     [101   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 10\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[600   0]\n",
    "     [103   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.88\n",
    "    [[569  31]\n",
    "     [ 56  47]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.001} 0.790853658537 [mean: 0.79085, std: 0.05225, params: {'C': 0.001}, mean: 0.79024, std: 0.05004, params: {'C': 0.01}, mean: 0.79024, std: 0.05004, params: {'C': 0.1}, mean: 0.79024, std: 0.05004, params: {'C': 1.0}, mean: 0.79024, std: 0.05004, params: {'C': 10.0}, mean: 0.79024, std: 0.05004, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[600   0]\n",
    "     [103   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 11\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[600   0]\n",
    "     [103   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.87\n",
    "    [[572  28]\n",
    "     [ 61  42]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.01} 0.798780487805 [mean: 0.79451, std: 0.06507, params: {'C': 0.001}, mean: 0.79878, std: 0.06653, params: {'C': 0.01}, mean: 0.79878, std: 0.06653, params: {'C': 0.1}, mean: 0.79878, std: 0.06653, params: {'C': 1.0}, mean: 0.79878, std: 0.06653, params: {'C': 10.0}, mean: 0.79878, std: 0.06653, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[600   0]\n",
    "     [103   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 12\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[600   0]\n",
    "     [103   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.86\n",
    "    [[555  45]\n",
    "     [ 53  50]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.01} 0.810365853659 [mean: 0.80976, std: 0.05545, params: {'C': 0.001}, mean: 0.81037, std: 0.05662, params: {'C': 0.01}, mean: 0.81037, std: 0.05662, params: {'C': 0.1}, mean: 0.81037, std: 0.05662, params: {'C': 1.0}, mean: 0.81037, std: 0.05662, params: {'C': 10.0}, mean: 0.81037, std: 0.05662, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[600   0]\n",
    "     [103   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 13\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[596   0]\n",
    "     [107   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.87\n",
    "    [[564  32]\n",
    "     [ 59  48]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.001} 0.808536585366 [mean: 0.80854, std: 0.04830, params: {'C': 0.001}, mean: 0.80793, std: 0.05322, params: {'C': 0.01}, mean: 0.80793, std: 0.05322, params: {'C': 0.1}, mean: 0.80793, std: 0.05322, params: {'C': 1.0}, mean: 0.80793, std: 0.05322, params: {'C': 10.0}, mean: 0.80793, std: 0.05322, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[596   0]\n",
    "     [107   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 14\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.84\n",
    "    Accuracy on test data:     0.88\n",
    "    [[616   0]\n",
    "     [ 87   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.88\n",
    "    [[585  31]\n",
    "     [ 54  33]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.001} 0.776219512195 [mean: 0.77622, std: 0.06963, params: {'C': 0.001}, mean: 0.77073, std: 0.06534, params: {'C': 0.01}, mean: 0.77073, std: 0.06534, params: {'C': 0.1}, mean: 0.77073, std: 0.06534, params: {'C': 1.0}, mean: 0.77073, std: 0.06534, params: {'C': 10.0}, mean: 0.77073, std: 0.06534, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.84\n",
    "    Accuracy on test data:     0.88\n",
    "    [[616   0]\n",
    "     [ 87   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 15\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[599   0]\n",
    "     [104   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.86\n",
    "    [[559  40]\n",
    "     [ 60  44]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.001} 0.787804878049 [mean: 0.78780, std: 0.06997, params: {'C': 0.001}, mean: 0.78415, std: 0.06799, params: {'C': 0.01}, mean: 0.78415, std: 0.06799, params: {'C': 0.1}, mean: 0.78415, std: 0.06799, params: {'C': 1.0}, mean: 0.78415, std: 0.06799, params: {'C': 10.0}, mean: 0.78415, std: 0.06799, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[599   0]\n",
    "     [104   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 16\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.84\n",
    "    [[589   0]\n",
    "     [114   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.86\n",
    "    [[556  33]\n",
    "     [ 67  47]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.01} 0.821341463415 [mean: 0.81890, std: 0.04598, params: {'C': 0.001}, mean: 0.82134, std: 0.05008, params: {'C': 0.01}, mean: 0.82134, std: 0.05008, params: {'C': 0.1}, mean: 0.82134, std: 0.05008, params: {'C': 1.0}, mean: 0.82134, std: 0.05008, params: {'C': 10.0}, mean: 0.82134, std: 0.05008, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.84\n",
    "    [[589   0]\n",
    "     [114   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 17\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[599   0]\n",
    "     [104   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.89\n",
    "    [[577  22]\n",
    "     [ 58  46]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.001} 0.785365853659 [mean: 0.78537, std: 0.07928, params: {'C': 0.001}, mean: 0.78537, std: 0.08049, params: {'C': 0.01}, mean: 0.78537, std: 0.08049, params: {'C': 0.1}, mean: 0.78537, std: 0.08049, params: {'C': 1.0}, mean: 0.78537, std: 0.08049, params: {'C': 10.0}, mean: 0.78537, std: 0.08049, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[599   0]\n",
    "     [104   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 18\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.84\n",
    "    Accuracy on test data:     0.87\n",
    "    [[615   0]\n",
    "     [ 88   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.89\n",
    "    [[588  27]\n",
    "     [ 49  39]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.01} 0.765243902439 [mean: 0.76402, std: 0.05819, params: {'C': 0.001}, mean: 0.76524, std: 0.05911, params: {'C': 0.01}, mean: 0.76524, std: 0.05911, params: {'C': 0.1}, mean: 0.76524, std: 0.05911, params: {'C': 1.0}, mean: 0.76524, std: 0.05911, params: {'C': 10.0}, mean: 0.76524, std: 0.05911, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.84\n",
    "    Accuracy on test data:     0.87\n",
    "    [[615   0]\n",
    "     [ 88   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 19\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[595   0]\n",
    "     [108   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.86\n",
    "    [[557  38]\n",
    "     [ 62  46]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.01} 0.786585365854 [mean: 0.78476, std: 0.07214, params: {'C': 0.001}, mean: 0.78659, std: 0.07204, params: {'C': 0.01}, mean: 0.78659, std: 0.07204, params: {'C': 0.1}, mean: 0.78659, std: 0.07204, params: {'C': 1.0}, mean: 0.78659, std: 0.07204, params: {'C': 10.0}, mean: 0.78659, std: 0.07204, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.85\n",
    "    Accuracy on test data:     0.85\n",
    "    [[595   0]\n",
    "     [108   0]]\n",
    "    ########################################################\n",
    "    \n",
    "    snps 20\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.84\n",
    "    Accuracy on test data:     0.87\n",
    "    [[609   0]\n",
    "     [ 94   0]]\n",
    "    ########################################################\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.00\n",
    "    Accuracy on test data:     0.88\n",
    "    [[575  34]\n",
    "     [ 47  47]]\n",
    "    ########################################################\n",
    "    LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
    "         intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
    "         penalty='l2', random_state=None, tol=0.0001, verbose=0) {'C': 0.001} 0.791463414634 [mean: 0.79146, std: 0.05793, params: {'C': 0.001}, mean: 0.79024, std: 0.05853, params: {'C': 0.01}, mean: 0.79024, std: 0.05853, params: {'C': 0.1}, mean: 0.79024, std: 0.05853, params: {'C': 1.0}, mean: 0.79024, std: 0.05853, params: {'C': 10.0}, mean: 0.79024, std: 0.05853, params: {'C': 100.0}]\n",
    "    using reuse split\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.84\n",
    "    Accuracy on test data:     0.87\n",
    "    [[609   0]\n",
    "     [ 94   0]]\n",
    "    ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coeficcients(df_names):\n",
    "    counter=0\n",
    "    coef_dict={}\n",
    "    for df_name in df_names:\n",
    "        counter+=1\n",
    "        \n",
    "        print ('snps '+str(counter))\n",
    "        \n",
    "        merged_df=pd.read_csv(df_name, index_col=0)\n",
    "        \n",
    "        lcols=list(merged_df)\n",
    "        lcols.remove('fast')\n",
    "        \n",
    "        itrain, itest = train_test_split(xrange(merged_df.shape[0]), train_size=0.7)\n",
    "        mask=np.ones(merged_df.shape[0], dtype='int')\n",
    "        mask[itrain]=1\n",
    "        mask[itest]=0\n",
    "        mask = (mask==1)\n",
    "        mask.shape, mask.sum()\n",
    "        \n",
    "        clfsvm_final, Xtrain, ytrain, Xtest, ytest = do_classify(LinearSVC(loss=\"hinge\"), {\"C\": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, merged_df,lcols, 'fast',1, mask=mask)\n",
    "        coefs=dict(zip(merged_df.columns.values[1:],clfsvm_final.coef_[0]))\n",
    "        coef_dict.update(coefs)\n",
    "    return coef_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs=get_coeficcients(upd_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    snps 1\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8578\n",
    "    [[561  39]\n",
    "     [ 61  42]]\n",
    "    ########################################################\n",
    "    snps 2\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8777\n",
    "    [[577  35]\n",
    "     [ 51  40]]\n",
    "    ########################################################\n",
    "    snps 3\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8791\n",
    "    [[575  28]\n",
    "     [ 57  43]]\n",
    "    ########################################################\n",
    "    snps 4\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8791\n",
    "    [[573  43]\n",
    "     [ 42  45]]\n",
    "    ########################################################\n",
    "    snps 5\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8535\n",
    "    [[556  32]\n",
    "     [ 71  44]]\n",
    "    ########################################################\n",
    "    snps 6\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.9988\n",
    "    Accuracy on test data:     0.8634\n",
    "    [[551  30]\n",
    "     [ 66  56]]\n",
    "    ########################################################\n",
    "    snps 7\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8691\n",
    "    [[563  37]\n",
    "     [ 55  48]]\n",
    "    ########################################################\n",
    "    snps 8\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.9976\n",
    "    Accuracy on test data:     0.8734\n",
    "    [[560  27]\n",
    "     [ 62  54]]\n",
    "    ########################################################\n",
    "    snps 9\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.9994\n",
    "    Accuracy on test data:     0.8848\n",
    "    [[580  27]\n",
    "     [ 54  42]]\n",
    "    ########################################################\n",
    "    snps 10\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.9976\n",
    "    Accuracy on test data:     0.8606\n",
    "    [[560  38]\n",
    "     [ 60  45]]\n",
    "    ########################################################\n",
    "    snps 11\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8791\n",
    "    [[562  25]\n",
    "     [ 60  56]]\n",
    "    ########################################################\n",
    "    snps 12\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.9994\n",
    "    Accuracy on test data:     0.8663\n",
    "    [[550  41]\n",
    "     [ 53  59]]\n",
    "    ########################################################\n",
    "    snps 13\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.9988\n",
    "    Accuracy on test data:     0.8720\n",
    "    [[572  32]\n",
    "     [ 58  41]]\n",
    "    ########################################################\n",
    "    snps 14\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8890\n",
    "    [[575  29]\n",
    "     [ 49  50]]\n",
    "    ########################################################\n",
    "    snps 15\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8578\n",
    "    [[559  30]\n",
    "     [ 70  44]]\n",
    "    ########################################################\n",
    "    snps 16\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8777\n",
    "    [[564  29]\n",
    "     [ 57  53]]\n",
    "    ########################################################\n",
    "    snps 17\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8805\n",
    "    [[563  35]\n",
    "     [ 49  56]]\n",
    "    ########################################################\n",
    "    snps 18\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.9988\n",
    "    Accuracy on test data:     0.8634\n",
    "    [[555  43]\n",
    "     [ 53  52]]\n",
    "    ########################################################\n",
    "    snps 19\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 1.0000\n",
    "    Accuracy on test data:     0.8578\n",
    "    [[563  22]\n",
    "     [ 78  40]]\n",
    "    ########################################################\n",
    "    snps 20\n",
    "    using mask\n",
    "    ############# based on standard predict ################\n",
    "    Accuracy on training data: 0.9994\n",
    "    Accuracy on test data:     0.8805\n",
    "    [[568  25]\n",
    "     [ 59  51]]\n",
    "    ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs_df=pd.DataFrame(coefs, index=['coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs_df=coefs_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_names = []\n",
    "for name in coefs_df.index.values:\n",
    "    i=0\n",
    "    for c in name:\n",
    "        i+=1\n",
    "        if c=='_':\n",
    "            upd_name=int(name[i:])\n",
    "            break\n",
    "    new_names.append(upd_name)\n",
    "\n",
    "coefs_df.loc[:,'name']=(new_names)\n",
    "coefs_df.set_index('name',inplace=True)\n",
    "coefs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(coefs_df.index.values,abs(coefs_df['coefficient']),alpha=.1)\n",
    "plt.axis([0,np.max(coefs_df.index.values),0,.03]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs_df.to_csv('coefs_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_snps(dfs):\n",
    "    counter=0\n",
    "\n",
    "    for df_name in dfs:\n",
    "        \n",
    "        merged_df=pd.read_csv(df_name, index_col=0)\n",
    "        if counter ==0:\n",
    "            finaldf=pd.DataFrame(index=list(merged_df.index))\n",
    "            finaldf=finaldf.transpose()                      \n",
    "        counter+=1\n",
    "        merged_df=merged_df.transpose()\n",
    "        if counter != 1:\n",
    "            merged_df.drop(['fast'], inplace=True)\n",
    "        frames=[finaldf,merged_df]\n",
    "        finaldf=pd.concat(frames)\n",
    "    finaldf=finaldf.transpose()\n",
    "    return finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finaldf=combine_snps(upd_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finaldf.to_csv('tempdata/finally_totals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snp_cols = list(finaldf.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrs = []\n",
    "for i in snp_cols:\n",
    "    corr = finaldf.loc[:, [i, 'fast']].corr(method=\"spearman\") \n",
    "    corrs.append(corr.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.abs(corrs), '.', markersize=4, linestyle='None');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "snp_cols = list(finaldf.columns[1:])\n",
    "\n",
    "spearman_cor_dict = {}\n",
    "p_val_dict = {}\n",
    "neg_log_p_val_dict = {}\n",
    "for snp in snp_cols:\n",
    "    cor,p = spearmanr(list(finaldf['fast']),list(finaldf[snp]))\n",
    "    spearman_cor_dict.update({snp:(cor)}) \n",
    "    p_val_dict.update({snp:(p)})\n",
    "    pp = -np.log10(p)\n",
    "    neg_log_p_val_dict.update({snp:(pp)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats_df=pd.DataFrame(spearman_cor_dict, index=['spearman_cor']).append(pd.DataFrame(p_val_dict, index=['p_vals'])).append(pd.DataFrame(neg_log_p_val_dict, index=['neg_log_p_vals']))\n",
    "stats_df=stats_df.transpose()\n",
    "\n",
    "new_names = []\n",
    "for name in stats_df.index.values:\n",
    "    i=0\n",
    "    for c in name:\n",
    "        i+=1\n",
    "        if c=='_':\n",
    "            upd_name=int(name[i:])\n",
    "            break=\n",
    "    new_names.append(upd_name)\n",
    "\n",
    "stats_df.loc[:,'name']=(new_names)\n",
    "stats_df.set_index('name',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(stats_df.index.values,stats_df['neg_log_p_vals'],alpha=.15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(stats_df.index.values,stats_df['spearman_cor'],alpha=.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(stats_df.index.values,abs(stats_df['spearman_cor']),alpha=.1)\n",
    "plt.axis([0,np.max(stats_df.index.values),0,.6]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats_df.to_csv('all_stats_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
